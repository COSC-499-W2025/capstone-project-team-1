# ğŸ—ï¸ ArtifactMiner

> Automated portfolio and resume generation from your coding projects

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ Table of Contents

- [Team Information](#-team-information)
- [Project Overview](#-project-overview)
- [Features](#-features)
- [System Architecture](#-system-architecture)
- [Data Flow Diagrams](#-data-flow-diagrams)
- [Technology Stack](#-technology-stack)
- [Project Structure](#-project-structure)
- [API Reference](#-api-reference)
- [Database Schema](#-database-schema)
- [Analysis Pipeline](#-analysis-pipeline)
- [Getting Started](#-getting-started)
- [Usage](#-usage)
- [Work Breakdown Structure](#-work-breakdown-structure)
- [Contributing](#-contributing)
- [Database Migrations](#-database-migrations)

---

## ğŸ‘¥ Team Information

**Team Number:** 1

| Team Member | Student Number | Primary Responsibilities |
|-------------|----------------|--------------------------|
| Shlok Shah | 50732213 | TUI & UX, Skills Extraction |
| Brendan James | 31927486 | Metrics & Ranking |
| Ahmad Memon | 61846432 | TUI & UX, Demo Script |
| Stavan Shah | 43960608 | API Gateway, Database, Skills |
| Evan Crowley | 82710823 | Repository Intelligence |
| Nathan Helm | 68837038 | Ingestion & Crawler |

---

## ğŸ¯ Project Overview

**ArtifactMiner** is an automated portfolio and resume generation system designed to help CS students, TAs, and career advisors create professional work portfolios from past coding projects.

### The Problem

Creating a comprehensive resume that accurately reflects your technical skills and project contributions is time-consuming and often incomplete. Developers have years of work scattered across various projects, but extracting and presenting this information effectively is challenging.

### Our Solution

ArtifactMiner automates the entire process:

1. **Upload** your projects as a ZIP file
2. **Analyze** Git history, code patterns, and dependencies
3. **Extract** skills from your actual code contributions
4. **Generate** a professional portfolio or resume

### Primary Users

- **CS Students** - Building resumes for internships and job applications
- **TAs and Career Advisors** - Helping students showcase their work
- **Developers** - Creating portfolios from existing projects

### Typical Workflow

1. User launches the TUI (Terminal User Interface)
2. Configures preferences and provides consent for data processing
3. Uploads a ZIP file containing their projects
4. System crawls and analyzes all Git repositories
5. Skills are extracted and ranked by contribution
6. A professional resume/portfolio is generated

---

## âœ¨ Features

### Core Features

- **ğŸ”’ Privacy-First Design**
  - Three consent levels: `full` (with LLM), `no_llm` (offline only), `none`
  - LLM features are gated behind explicit user consent
  - All data processing is local by default

- **ğŸ“Š Offline-First Analysis**
  - Heuristic skill extraction from code patterns
  - Framework detection from manifest files (package.json, requirements.txt, etc.)
  - Template-based summaries when LLM is disabled

- **ğŸ‘¤ User Attribution**
  - Skills attributed to specific users in collaborative repos
  - Contribution percentages calculated from Git history
  - Email-based author identification

- **ğŸ” Comprehensive Skill Detection**
  - Language patterns (Python, JavaScript, Java, Go)
  - Framework dependencies (FastAPI, React, Django, Spring Boot, etc.)
  - Code patterns (async programming, error handling, resource management)
  - Higher-order insights (architecture, robustness, complexity awareness)

- **ğŸ“ˆ Project Ranking**
  - Projects ranked by user contribution percentage
  - Commit frequency analysis
  - Activity timeline generation

- **ğŸ¤– Optional LLM Integration**
  - Polished AI-generated contribution summaries
  - Resume bullet point generation
  - Consent-gated for privacy

---

## ğŸ›ï¸ System Architecture

The system follows a layered architecture with clear separation of concerns:

```mermaid
flowchart TB
    subgraph TUI["ğŸ–¥ï¸ TUI - Terminal User Interface"]
        UP["User Preferences\nâ€¢ Email/Profile\nâ€¢ Consent Level\nâ€¢ File Filters"]
        AD["Analysis Dashboard\nâ€¢ Progress Updates\nâ€¢ Results Display\nâ€¢ Resume Preview"]
    end

    subgraph API["âš¡ FastAPI Backend"]
        CR[Consent Router]
        ZR[ZIP Router]
        AR[Analyze Router]
        RR[Retrieval Router]
    end

    subgraph CORE["ğŸ”§ Core Processing Modules"]
        DC["Directory Crawler\nâ€¢ ZIP Extract\nâ€¢ File Walk\nâ€¢ MIME Detect\nâ€¢ Deduplication"]
        RI["Repository Intelligence\nâ€¢ Git Analysis\nâ€¢ Lang Detection\nâ€¢ Framework Detect\nâ€¢ Contribution Stats"]
        SE["Skills Extraction\nâ€¢ Patterns\nâ€¢ Dependencies\nâ€¢ Insights\nâ€¢ Profile"]
        PR["Project Ranker\nâ€¢ Contribution\nâ€¢ Timeline\nâ€¢ Scoring"]
        LLM["LLM Integration\nâ€¢ OpenAI API\nâ€¢ Consent-Gated\nâ€¢ Summary Gen"]
        RG["Resume Generator\nâ€¢ Insights\nâ€¢ Summaries\nâ€¢ Export"]
    end

    subgraph DB["ğŸ’¾ SQLite Database - Alembic Managed"]
        Tables[("Questions â€¢ Answers â€¢ Consent<br/>RepoStats â€¢ Skills â€¢ ResumeItems")]
    end

    TUI --> API
    API --> CORE
    DC --> RI --> SE
    SE --> PR --> LLM --> RG
    CORE --> DB
```

### Component Descriptions

| Component | Description |
|-----------|-------------|
| **TUI** | Textual-based terminal interface for user interaction |
| **User Preferences** | Configuration screen for email, goals, and consent |
| **Analysis Dashboard** | Real-time progress and results display |
| **FastAPI Backend** | REST API exposing all backend services |
| **Directory Crawler** | ZIP extraction, file traversal, and indexing |
| **Repository Intelligence** | Git history analysis and contribution stats |
| **Skills Extraction** | Pattern-based and dependency-based skill detection |
| **Project Ranker** | Contribution-based project scoring |
| **LLM Integration** | Optional OpenAI-powered summary generation |
| **Resume Generator** | Final portfolio assembly and export |
| **SQLite Database** | Persistent storage with Alembic migrations |

---

## ğŸ“Š Data Flow Diagrams

### Level 0 - Context Diagram

```mermaid
flowchart LR
    User["ğŸ‘¤ User"]
    TUI["ğŸ–¥ï¸ TUI\n(ArtifactMiner)"]
    
    User -->|"Add files/directories"| TUI
    User -->|"List interests/target jobs"| TUI
    TUI -->|"Return generated resume"| User
```

### Level 1 - System Diagram

```mermaid
flowchart TB
    User["ğŸ‘¤ User"]
    
    subgraph TUI["ğŸ–¥ï¸ TUI"]
        Prefs["Preferences"]
        Dashboard["Dashboard"]
        Export["Export"]
    end
    
    FS[("ğŸ“ File System\n(ZIP/Dirs)")]
    
    subgraph Crawler["ğŸ” Crawler"]
        Extract["Extract ZIP"]
        Traverse["Traverse Files"]
        Discover["Repo Discovery"]
    end
    
    subgraph LLM["ğŸ¤– LLM (Optional)"]
        SummaryGen["Summary Generation"]
        ResumePolish["Resume Polish"]
        ConsentGated["Consent-Gated"]
    end
    
    DB[("ğŸ’¾ Local SQLite DB\nâ€¢ RepoStats\nâ€¢ Skills\nâ€¢ Summaries\nâ€¢ ResumeItems")]
    
    User -->|"Add files/dirs"| TUI
    TUI -->|"Return resume"| User
    User -->|"Save Formatted"| FS
    
    TUI -->|"User Preferences\nProgress Updates"| Crawler
    FS -->|"Files"| Crawler
    Crawler -->|"Request directory"| FS
    
    Crawler -->|"Parsed Files Data\nRepo Stats"| LLM
    TUI -->|"Formatted info"| LLM
    
    Crawler -->|"Saves Previous Runs"| DB
    DB -->|"Fetches Previous Runs"| Crawler
```

### Data Interactions

1. **User â†” TUI**: User configures preferences (email, consent level, file filters) and receives the generated resume/portfolio.

2. **TUI â†” Crawler**: TUI sends user preferences; Crawler returns progress updates and analysis results.

3. **File System â†” Crawler**: Crawler extracts ZIP files and traverses directories to discover Git repositories.

4. **Crawler â†” LLM**: When consent allows, parsed data is sent to OpenAI for polished summary generation.

5. **Crawler â†” Database**: All analysis results are persisted for retrieval and future reference.

6. **Database â†” TUI**: Previous analysis runs can be retrieved for comparison or export.

---

## ğŸ› ï¸ Technology Stack

| Category | Technology | Purpose |
|----------|------------|---------|
| **Backend Framework** | FastAPI | REST API with async support |
| **ASGI Server** | Uvicorn | High-performance HTTP server |
| **Database** | SQLite | Lightweight local persistence |
| **ORM** | SQLAlchemy 2.0 | Database abstraction |
| **Migrations** | Alembic | Schema version control |
| **Data Validation** | Pydantic 2 | Request/response models |
| **TUI Framework** | Textual | Rich terminal interfaces |
| **HTTP Client** | httpx | Async HTTP requests |
| **Git Analysis** | GitPython | Repository traversal |
| **LLM Integration** | OpenAI API | Optional summary generation |
| **Testing** | pytest | Unit and integration tests |
| **Async Testing** | pytest-asyncio | Async test support |
| **Package Manager** | uv | Fast Python package installer |

---

## ğŸ“ Project Structure

```
capstone-project-team-1/
â”œâ”€â”€ src/artifactminer/              # Main source code
â”‚   â”œâ”€â”€ api/                        # FastAPI endpoints
â”‚   â”‚   â”œâ”€â”€ app.py                  # Application factory
â”‚   â”‚   â”œâ”€â”€ analyze.py              # Master orchestration (â­ main pipeline)
â”‚   â”‚   â”œâ”€â”€ consent.py              # Privacy consent management
â”‚   â”‚   â”œâ”€â”€ crawler.py              # Directory crawler endpoints
â”‚   â”‚   â”œâ”€â”€ openai.py               # LLM integration
â”‚   â”‚   â”œâ”€â”€ projects.py             # Project management
â”‚   â”‚   â”œâ”€â”€ retrieval.py            # Read-only data retrieval
â”‚   â”‚   â”œâ”€â”€ schemas.py              # Pydantic models (API contracts)
â”‚   â”‚   â””â”€â”€ zip.py                  # ZIP upload handling
â”‚   â”‚
â”‚   â”œâ”€â”€ db/                         # Database layer
â”‚   â”‚   â”œâ”€â”€ models.py               # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ database.py             # Connection management
â”‚   â”‚   â””â”€â”€ seed.py                 # Default data seeding
â”‚   â”‚
â”‚   â”œâ”€â”€ RepositoryIntelligence/     # Git repository analysis
â”‚   â”‚   â”œâ”€â”€ repo_intelligence_main.py    # Core repo statistics
â”‚   â”‚   â”œâ”€â”€ repo_intelligence_user.py    # User contribution analysis
â”‚   â”‚   â”œâ”€â”€ repo_intelligence_AI.py      # AI-powered insights
â”‚   â”‚   â”œâ”€â”€ framework_detector.py        # Framework detection
â”‚   â”‚   â””â”€â”€ activity_classifier.py       # Activity classification
â”‚   â”‚
â”‚   â”œâ”€â”€ skills/                     # Skill extraction module
â”‚   â”‚   â”œâ”€â”€ skill_extractor.py      # Main extraction logic
â”‚   â”‚   â”œâ”€â”€ skill_patterns.py       # Code pattern definitions
â”‚   â”‚   â”œâ”€â”€ deep_analysis.py        # Higher-order insights
â”‚   â”‚   â”œâ”€â”€ persistence.py          # Database helpers
â”‚   â”‚   â”œâ”€â”€ user_profile.py         # User skill profiles
â”‚   â”‚   â””â”€â”€ signals/                # Detection signals
â”‚   â”‚       â”œâ”€â”€ code_signals.py     # Code pattern signals
â”‚   â”‚       â”œâ”€â”€ dependency_signals.py   # Dependency signals
â”‚   â”‚       â”œâ”€â”€ language_signals.py     # Language detection
â”‚   â”‚       â””â”€â”€ file_signals.py         # File-based signals
â”‚   â”‚
â”‚   â”œâ”€â”€ directorycrawler/           # File system crawler
â”‚   â”‚   â”œâ”€â”€ directory_walk.py       # Main crawler logic
â”‚   â”‚   â”œâ”€â”€ zip_file_handler.py     # ZIP extraction
â”‚   â”‚   â”œâ”€â”€ store_file_dict.py      # File storage
â”‚   â”‚   â””â”€â”€ check_file_duplicate.py # SHA-256 deduplication
â”‚   â”‚
â”‚   â”œâ”€â”€ tui/                        # Terminal User Interface
â”‚   â”‚   â”œâ”€â”€ app.py                  # Main TUI application
â”‚   â”‚   â””â”€â”€ screens/                # UI screens
â”‚   â”‚       â”œâ”€â”€ welcome.py          # Welcome screen
â”‚   â”‚       â”œâ”€â”€ consent.py          # Consent configuration
â”‚   â”‚       â”œâ”€â”€ userconfig.py       # User preferences
â”‚   â”‚       â”œâ”€â”€ upload.py           # ZIP upload
â”‚   â”‚       â”œâ”€â”€ list_contents.py    # Directory listing
â”‚   â”‚       â””â”€â”€ file_browser.py     # File browser
â”‚   â”‚
â”‚   â”œâ”€â”€ helpers/                    # Shared utilities
â”‚   â”‚   â”œâ”€â”€ project_ranker.py       # Contribution-based ranking
â”‚   â”‚   â””â”€â”€ openai.py               # OpenAI client wrapper
â”‚   â”‚
â”‚   â””â”€â”€ mappings.py                 # Skill/framework mappings
â”‚
â”œâ”€â”€ tests/                          # Test suite
â”‚   â”œâ”€â”€ api/                        # API endpoint tests
â”‚   â”œâ”€â”€ db/                         # Database tests
â”‚   â”œâ”€â”€ directorycrawler/           # Crawler tests
â”‚   â”œâ”€â”€ Repository-Intelligence-tests/  # Repo analysis tests
â”‚   â””â”€â”€ tui/                        # TUI tests
â”‚
â”œâ”€â”€ alembic/                        # Database migrations
â”‚   â”œâ”€â”€ versions/                   # Migration scripts
â”‚   â””â”€â”€ env.py                      # Alembic configuration
â”‚
â”œâ”€â”€ demo.py                         # Rich-powered CLI demo
â”œâ”€â”€ pyproject.toml                  # Package configuration
â””â”€â”€ README.md                       # This file
```

---

## ğŸ”Œ API Reference

### System Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check / readiness probe |

### Configuration Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/questions` | GET | Fetch configuration questions |
| `/answers` | POST | Submit user answers (email, goals) |
| `/consent` | GET | Get current consent level |
| `/consent` | PUT | Update consent level (full/no_llm/none) |

### Upload & Ingestion Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/zip/upload` | POST | Upload ZIP file for analysis |
| `/zip/{id}/directories` | GET | List directories in uploaded ZIP |
| `/crawler/files/{zip_id}` | GET | Get crawler file results |

### Analysis Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/analyze/{zip_id}` | POST | **Master orchestration** - full analysis pipeline |
| `/repos/analyze` | POST | Analyze single Git repository |
| `/openai/generate` | POST | Generate LLM summary (consent-gated) |

### Retrieval Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/skills/chronology` | GET | Skill timeline (oldest first) |
| `/resume` | GET | Resume items (newest first) |
| `/summaries` | GET | AI-generated contribution summaries |
| `/projects/timeline` | GET | Project timeline |
| `/projects/{id}` | DELETE | Soft-delete a project |

### Example Requests

```bash
# Health check
curl http://localhost:8000/health

# Upload ZIP file
curl -X POST "http://localhost:8000/zip/upload" \
  -F "file=@projects.zip"

# Run full analysis
curl -X POST "http://localhost:8000/analyze/1"

# Get skill chronology
curl http://localhost:8000/skills/chronology
```

---

## ğŸ’¾ Database Schema

### Entity Relationship Diagram

```mermaid
erDiagram
    Question ||--o{ UserAnswer : "has many"
    Question {
        int id PK
        string key UK
        string question_text
        int order
        bool required
        string answer_type
    }
    
    UserAnswer {
        int id PK
        int question_id FK
        string answer_text
        datetime answered_at
    }
    
    Consent {
        int id PK
        string consent_level
        datetime accepted_at
    }
    
    RepoStat ||--o{ ProjectSkill : "has many"
    RepoStat ||--o{ UserProjectSkill : "has many"
    RepoStat ||--o{ ResumeItem : "has many"
    RepoStat {
        int id PK
        string project_name
        string project_path
        bool is_collaborative
        json languages
        json frameworks
        datetime first_commit
        datetime last_commit
        int total_commits
        datetime deleted_at
    }
    
    UserRepoStat {
        int id PK
        string project_name
        string project_path
        string user_email
        int total_commits
        datetime first_commit
        datetime last_commit
        float contribution_pct
        float commit_frequency
        json activity_breakdown
    }
    
    UserAIntelligenceSummary {
        int id PK
        string repo_path
        string user_email
        text summary_text
        datetime generated_at
    }
    
    Skill ||--o{ ProjectSkill : "referenced by"
    Skill ||--o{ UserProjectSkill : "referenced by"
    Skill {
        int id PK
        string name UK
        string category
        datetime created_at
    }
    
    ProjectSkill {
        int id PK
        int repo_stat_id FK
        int skill_id FK
        float proficiency
        json evidence
    }
    
    UserProjectSkill {
        int id PK
        int repo_stat_id FK
        int skill_id FK
        string user_email
        float proficiency
        json evidence
    }
    
    ResumeItem {
        int id PK
        string title
        text content
        string category
        int repo_stat_id FK
        datetime created_at
    }
    
    UploadedZip {
        int id PK
        string filename
        string path
        datetime uploaded_at
        string extraction_path
    }
```

### Key Models

| Model | Purpose |
|-------|---------|
| `Question` | Configuration questions (email, end goal, file filters) |
| `UserAnswer` | User responses to configuration questions |
| `Consent` | LLM consent level (full / no_llm / none) |
| `UploadedZip` | Tracked uploaded ZIP files |
| `RepoStat` | Repository-level statistics and metadata |
| `UserRepoStat` | User-specific contribution statistics |
| `Skill` | Master skill records (shared across projects) |
| `ProjectSkill` | Repo-level skill associations |
| `UserProjectSkill` | User-attributed skills (for collaborative repos) |
| `ResumeItem` | Generated resume bullet points and insights |
| `UserAIntelligenceSummary` | AI-generated contribution summaries |

---

## ğŸ”„ Analysis Pipeline

The **`/analyze/{zip_id}`** endpoint orchestrates the complete artifact mining pipeline:

```mermaid
flowchart TD
    subgraph PIPELINE["ğŸ”„ Analysis Pipeline"]
        A["ğŸ“¦ 1. EXTRACT ZIP<br/>Extract to ./extracted/zip_id/<br/>Validate file integrity"]
        B["ğŸ” 2. DISCOVER GIT REPOS<br/>Recursively find .git directories<br/>Identify project boundaries"]
        
        subgraph FOREACH["3. FOR EACH REPOSITORY"]
            C["ğŸ“Š a. Get Repo Stats<br/>â€¢ Languages & percentages<br/>â€¢ Framework detection<br/>â€¢ Commit timeline<br/>â€¢ Collaborative vs solo"]
            D["ğŸ‘¤ b. Get User Stats<br/>â€¢ Contribution percentage<br/>â€¢ Commit frequency<br/>â€¢ Activity breakdown<br/>â€¢ User's added lines"]
            E["ğŸ› ï¸ c. Extract Skills<br/>â€¢ Code pattern matching<br/>â€¢ Dependency analysis<br/>â€¢ User-attributed skills"]
            F["ğŸ§  d. Deep Analysis<br/>â€¢ Higher-order insights<br/>â€¢ Complexity awareness<br/>â€¢ API design patterns"]
            G["ğŸ¤– e. Generate Summaries<br/>if consent = 'full'<br/>â€¢ LLM-powered summaries<br/>â€¢ Resume bullet points"]
        end
        
        H["ğŸ† 4. RANK PROJECTS<br/>Sort by contribution %<br/>Calculate engagement scores"]
        I["ğŸ’¾ 5. PERSIST DATA<br/>Save to SQLite<br/>Create resume items"]
        J["âœ… 6. RETURN RESULTS<br/>Comprehensive AnalyzeResponse<br/>Ready for display or export"]
    end
    
    A --> B --> FOREACH
    C --> D --> E --> F --> G
    FOREACH --> H --> I --> J
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.11 or higher
- [uv](https://github.com/astral-sh/uv) package manager (recommended)
- Git

### Installation

```bash
# Clone the repository
git clone https://github.com/COSC-499-W2025/capstone-project-team-1.git
cd capstone-project-team-1

# Install dependencies with uv
uv sync

# Apply database migrations
uv run alembic upgrade head
```

### Environment Setup (Optional)

Create a `.env` file for LLM integration:

```env
OPENAI_API_KEY=your_api_key_here
```

---

## ğŸ“– Usage

### Option 1: Terminal User Interface (TUI)

```bash
# Start the API server (in terminal 1)
uv run api

# Launch the TUI (in terminal 2)
uv run artifactminer-tui
```

### Option 2: CLI Demo

The demo script provides a rich terminal experience showcasing all API features:

```bash
# Make sure the API is running
uv run api

# Run the demo (in another terminal)
uv run python demo.py
```

### Option 3: Direct API Usage

```bash
# Start the API server
uv run api

# API is available at http://localhost:8000
# Interactive docs at http://localhost:8000/docs
```

### Typical Workflow

1. **Configure** - Answer questions about your email and career goals
2. **Consent** - Select your privacy preference (full/no_llm/none)
3. **Upload** - Upload a ZIP file containing your projects
4. **Analyze** - Run the analysis pipeline
5. **Review** - View extracted skills, project rankings, and insights
6. **Export** - Download your generated portfolio/resume

---

## ğŸ“‹ Work Breakdown Structure

### 1. TUI & UX â€” Owner: Ahmad

| Requirement | Description | Difficulty |
|-------------|-------------|------------|
| Consent & LLM Dialogs | Capture data-access consent and optional LLM permission | Easy |
| Ingest Wizard | Select .zip, show validation errors, progress/cancel | Medium |
| Results Panels | Views for projects, skills, reports | Medium |
| Retrieve & Delete | Fetch prior report/rÃ©sumÃ© items; confirm safe delete | Easy |
| TUI Starter Kit | App frame, panel template, API helper | Medium |

### 2. API Gateway, Config & Persistence â€” Owner: Stavan

| Requirement | Description | Difficulty |
|-------------|-------------|------------|
| Contracts & Stubs | FastAPI endpoints & Pydantic schemas | Medium |
| Consent Enforcement | Block processing until consent | Easy |
| Config Store | Persist user emails, target role, prefs | Medium |
| DB & Migrations | SQLite models, Alembic migrations, CRUD | Medium |
| Exports & Safe Delete | JSON/CSV exports; ref-counted deletes | Medium |
| PyPI Package & CLI | PyPI-ready pyproject.toml, CLI commands | Easy |

### 3. Ingestion, Classification & Dedupe â€” Owner: Nathan

| Requirement | Description | Difficulty |
|-------------|-------------|------------|
| Zip Validation | Accept .zip; wrong-format error handling | Easy |
| Secure Unzip | Safe extraction (no zip-slip), size limits | Medium |
| Traverse & Index | Walk files/dirs; capture paths, timestamps | Medium |
| MIME & Buckets | Detect type; bucket into code/test/docs | Medium |
| SHA-256 Dedupe | Hash artifacts for duplicate detection | Medium |
| Artifacts API | GET /ingest/{id}/artifacts returns metadata | Easy |

### 4. Repository Intelligence â€” Owner: Evan

| Requirement | Description | Difficulty |
|-------------|-------------|------------|
| Repo Discovery | Identify project boundaries (.git or manifest) | Medium |
| Duration & Timeline | Firstâ†”last commit timestamps per project | Medium |
| Lang & Framework | Primary language + framework via manifests | Medium |
| Collab vs Solo | Flag collaborative projects | Medium |
| Contribution Estimation | Commits/LOC/ownership shares for the user | Hard |
| Repo Stats API | POST /repos/analyze â†’ RepoStats | Easy |

### 5. Metrics, Ranking & Reporting â€” Owner: Brendan

| Requirement | Description | Difficulty |
|-------------|-------------|------------|
| Activity Ratios | Code/test/docs/design proportions | Medium |
| Role-Aware Ranking | Rank by contribution, role fit, recency | Medium |
| Chronology | Ordered list with durations | Easy |
| Report Assembly | Compose text/JSON outputs | Medium |
| Retrieval APIs | GET /projects, /projects/chronology | Easy |

### 6. Skills Extraction & Summarization â€” Owner: Shlok

| Requirement | Description | Difficulty |
|-------------|-------------|------------|
| Heuristic Skills | Extract from READMEs/commits/manifests | Medium |
| Skills Chronology | First/last-seen timestamps; timeline | Medium |
| Offline Summaries | Template/TextRank summaries | Medium |
| Optional LLM Summaries | Only if consented; metadata-only payloads | Hard |
| Skills/Summary APIs | /skills/extract â†’ SkillSignal[] | Easy |

---

## ğŸ¤ Contributing

### Development Setup

```bash
# Install dev dependencies
uv sync --dev

# Run tests
uv run pytest

# Run linting
uv run ruff check src/

# Type checking
uv run mypy src/
```

### Pull Request Guidelines

Please use the PR template at `.github/PULL_REQUEST_TEMPLATE.md`:

1. Describe your changes and link to the issue
2. Select the type of change
3. Document how you tested
4. Complete the checklist

---

## ğŸ“¦ Database Migrations

We use Alembic for schema version control. **Never manually edit `artifactminer.db`**.

### Keeping the Database Up to Date

```bash
# Apply all pending migrations
uv run alembic upgrade head
```

Run this after cloning and whenever you pull schema changes.

### Creating a New Migration

```bash
# 1. Update models in src/artifactminer/db/models.py

# 2. Generate migration script
uv run alembic revision --autogenerate -m "Describe your change"

# 3. Review generated file in alembic/versions/

# 4. Apply locally
uv run alembic upgrade head

# 5. Commit both model changes and migration file
```

### Downgrade / Testing

```bash
# Roll back one revision
uv run alembic downgrade -1

# Return to latest
uv run alembic upgrade head
```

### Starting Fresh

If you have an old database from before Alembic:

```bash
# Backup existing database (optional)
mv artifactminer.db artifactminer.db.bak

# Create fresh database with migrations
uv run alembic upgrade head
```

---

## ğŸ“„ License

This project is part of COSC 499 - Directed Studies at UBC Okanagan.

---

## ğŸ™ Acknowledgments

- COSC 499 Course Staff for guidance and support
- UBC Okanagan Computer Science Department
- Open source communities for the amazing tools we've built upon
